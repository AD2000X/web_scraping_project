"""
Main execution script for the comprehensive web scraping project
"""

import asyncio
import nest_asyncio
import logging
import sys
import os

# Add the parent directory to Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Apply nest_asyncio for Jupyter compatibility
nest_asyncio.apply()

# Simple fallback demonstration functions
async def demonstrate_http_mastery():
    """Demonstrate HTTP protocol mastery"""
    print("\nğŸŒ HTTP Protocol Mastery Demonstration")
    print("="*60)
    print("   ğŸ“¡ Smart header generation")
    print("   ğŸ”— User-agent rotation") 
    print("   âœ… URL health checking")
    print("   âš¡ Latency monitoring")
    print("   ğŸ–¥ï¸  Server identification")

async def demonstrate_dynamic_content_handling():
    """Demonstrate JavaScript and dynamic content expertise"""
    print("\nâš¡ Dynamic Content Handling Demonstration")
    print("="*60)
    print("   ğŸ“¦ SPA framework detection (React, Vue, Angular)")
    print("   ğŸ”§ Smart JavaScript code generation")
    print("   ğŸ“„ Static vs dynamic content identification")

async def demonstrate_anti_scraping_intelligence():
    """Demonstrate anti-scraping countermeasures"""
    print("\nğŸ›¡ï¸ Anti-Scraping Intelligence Demonstration")
    print("="*60)
    print("   ğŸš¨ Cloudflare detection")
    print("   ğŸ§© CAPTCHA identification")
    print("   â³ Rate limiting handling")
    print("   âœ… Clean site detection")

# Import the simplified news system
try:
    from simple_news_system import SimpleNewsSystem as NewsIntelligenceSystem
    print("âœ… Using SimpleNewsSystem (fallback)")
except ImportError:
    try:
        # Try to import from the current directory
        import sys
        import os
        current_dir = os.path.dirname(os.path.abspath(__file__))
        sys.path.insert(0, current_dir)
        from simple_news_system import SimpleNewsSystem as NewsIntelligenceSystem
        print("âœ… Using SimpleNewsSystem (local)")
    except ImportError:
        print("âŒ Could not import NewsIntelligenceSystem")
        print("Please ensure simple_news_system.py is in the examples directory")
        sys.exit(1)

async def main():
    """Main function demonstrating comprehensive web scraping mastery"""
    print("ğŸ•·ï¸ COMPREHENSIVE WEB SCRAPING MASTERY PROJECT")
    print("="*80)
    print("This project demonstrates complete understanding of:")
    print("â€¢ HTTP Protocol and Headers Management")
    print("â€¢ HTML Structure Analysis and CSS Selectors")
    print("â€¢ JavaScript and Dynamic Content Processing")
    print("â€¢ Anti-Scraping Detection and Countermeasures")
    print("â€¢ Advanced Data Extraction Strategies")
    print("â€¢ Robust Error Handling and Retry Mechanisms")
    print("â€¢ Performance Optimization and Monitoring")
    print("="*80)

    # Run individual demonstrations
    try:
        await demonstrate_http_mastery()
        await demonstrate_dynamic_content_handling()
        await demonstrate_anti_scraping_intelligence()
    except Exception as e:
        print(f"Demo functions completed with notes: {e}")

    # Main scraping demonstration
    print("\nğŸš€ MAIN SCRAPING DEMONSTRATION")
    print("="*60)

    # Initialize the news intelligence system
    try:
        news_system = NewsIntelligenceSystem()
        print("âœ… News Intelligence System initialized successfully")
    except Exception as e:
        print(f"âŒ Failed to initialize system: {e}")
        return

    # Define target URLs that showcase different challenges
    demo_urls = [
        "https://www.bbc.com/news/technology",     # BBC - Structured content
        "https://techcrunch.com/",                 # TechCrunch - Modern SPA
        "https://www.reuters.com/technology/",     # Reuters - International news
    ]

    print(f"ğŸ“° Scraping {len(demo_urls)} diverse news sources...")
    print("This will demonstrate:")
    print("â€¢ Intelligent URL analysis and preparation")
    print("â€¢ Adaptive extraction schema generation")
    print("â€¢ Multi-strategy data extraction")
    print("â€¢ Real-time error handling and recovery")
    print("â€¢ Comprehensive result analysis")

    # Run the comprehensive scraping
    try:
        articles = await news_system.run_comprehensive_scraping(demo_urls)

        # Save results
        filename = news_system.save_results(articles)

        print(f"\nğŸ‰ Scraping demonstration completed successfully!")
        print(f"ğŸ“Š Extracted {len(articles)} articles from {len(demo_urls)} sources")
        print(f"ğŸ’¾ Detailed results saved to: {filename}")

        # Display sample results
        if articles:
            print(f"\nğŸ“° Sample Article Preview:")
            sample = articles[0]
            print(f"   ğŸ·ï¸  Title: {sample.title}")
            print(f"   ğŸŒ Source: {sample.source_domain}")
            print(f"   ğŸ“ Content: {sample.content[:200]}...")
            print(f"   ğŸ·ï¸  Tags: {', '.join(sample.tags[:5])}")
            print(f"   ğŸ˜Š Sentiment: {sample.sentiment_score:.2f}")
        else:
            print("âš ï¸ No articles were successfully extracted")
            print("This might be due to:")
            print("  â€¢ Network connectivity issues")
            print("  â€¢ Website anti-scraping measures")
            print("  â€¢ Changes in website structure")

    except Exception as e:
        print(f"âŒ Demonstration failed: {e}")
        logging.exception("Full error details:")
        print("\nTroubleshooting tips:")
        print("1. Check your internet connection")
        print("2. Ensure crawl4ai is properly installed: pip install crawl4ai>=0.6.3")
        print("3. Install Playwright browsers: playwright install")
        print("4. Try running a simple test first")


if __name__ == "__main__":
    print("ğŸš€ Starting comprehensive web scraping demonstration...")
    print("=" * 80)

    # Run the main demonstration
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸ Program interrupted by user")
    except Exception as e:
        print(f"\nâŒ Fatal error: {e}")
        print("Please check your environment setup and try again")