"""
æµ‹è¯•å…·ä½“æ–‡ç« URLè€Œä¸æ˜¯ä¸»é¡µ
ä¿å­˜ä¸º: test_specific_urls.py
"""

import asyncio
from simple_news_system import SimpleNewsSystem

async def test_specific_articles():
    """æµ‹è¯•å…·ä½“çš„æ–°é—»æ–‡ç« URLè€Œä¸æ˜¯ä¸»é¡µ"""
    print("ğŸ§ª Testing Specific Article URLs...")
    
    system = SimpleNewsSystem()
    
    # ä½¿ç”¨å…·ä½“çš„æ–‡ç« URLè€Œä¸æ˜¯ä¸»é¡µ
    test_urls = [
        # BBCå…·ä½“æ–‡ç« 
        "https://www.bbc.com/news/articles/cz9j2k8k2nko",
        
        # æ›´å®¹æ˜“æŠ“å–çš„ç§‘æŠ€æ–°é—»ç½‘ç«™
        "https://arstechnica.com/",
        
        # The Verge (ä¹Ÿæ˜¯ç§‘æŠ€æ–°é—»ï¼Œä½†å¯èƒ½æ›´å®¹æ˜“æŠ“å–)
        "https://www.theverge.com/",
        
        # Hacker News (é€šå¸¸è¾ƒå®¹æ˜“æŠ“å–)
        "https://news.ycombinator.com/"
    ]
    
    print(f"ğŸ” Testing {len(test_urls)} diverse URLs...")
    
    all_articles = []
    
    for i, url in enumerate(test_urls, 1):
        print(f"\n--- Testing {i}/{len(test_urls)}: {url} ---")
        
        articles = await system.run_comprehensive_scraping([url])
        
        if articles and articles[0].content_length > 100:
            print(f"âœ… Success: {articles[0].content_length} characters")
            print(f"   Title: {articles[0].title[:60]}...")
            all_articles.extend(articles)
        else:
            print(f"âš ï¸ Limited content extracted")
    
    if all_articles:
        filename = system.save_results(all_articles)
        print(f"\nğŸ‰ Total successful extractions: {len(all_articles)}")
        print(f"ğŸ“ Results saved to: {filename}")
    else:
        print("\nâŒ No substantial content extracted from any URL")

if __name__ == "__main__":
    asyncio.run(test_specific_articles())